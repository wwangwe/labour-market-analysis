{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scrapping.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMJhoUTnFBgZ9J2Ojlg/ig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwangwe/labour-market-analysis/blob/working/notebooks/Web_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-time Kenyan Labour Market Analysis"
      ],
      "metadata": {
        "id": "Lo6D7R6TkFfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scrapping\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDqr0CMukVrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "utaEJak5HKon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVKqavvxWxjx"
      },
      "outputs": [],
      "source": [
        "headers = [\n",
        "    ({\n",
        "        'User-Agent':\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',\n",
        "    }),\n",
        "    ({\n",
        "        'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5)AppleWebKit/605.1.15 (KHTML, like Gecko)Version/12.1.1 Safari/605.1.15',\n",
        "    }),\n",
        "    ({\n",
        "        'User-Agent':\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
        "    }),\n",
        "    ({\n",
        "        'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.8 (KHTML, like Gecko)',\n",
        "    }),\n",
        "    ({\n",
        "        'User-Agent':\n",
        "        'Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 [FBAN/FBIOS;FBDV/iPhone9,1;FBMD/iPhone;FBSN/iOS;FBSV/13.5.1;FBSS/2;FBID/phone;FBLC/en_US;FBOP/5]'\n",
        "    })\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def header(headers: list) -> dict:\n",
        "    \"\"\"Generate a random header.\n",
        "\n",
        "    Args:\n",
        "        headers (list): List of headers.\n",
        "\n",
        "    Returns:\n",
        "        random_header (dict): Random header from the list of headers.\n",
        "    \"\"\"\n",
        "    random_int = randint(0, len(headers) - 1)\n",
        "    random_header = headers[random_int]\n",
        "    return random_header\n",
        "\n"
      ],
      "metadata": {
        "id": "bAtDKBPbQrJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_soup(url: str) -> 'BeautifulSoup':\n",
        "    \"\"\"Process url to a Beautiful Soup object.\n",
        "\n",
        "    Args:\n",
        "        url (str): Link to jobs page.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: Raised when requests.get fails.\n",
        "\n",
        "    Returns:\n",
        "        soup: Browsable bs4 object.\n",
        "    \"\"\"\n",
        "    response = requests.get(url, header(headers), timeout=5)\n",
        "    status_code = response.status_code\n",
        "    if status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup\n",
        "    else:\n",
        "        raise ValueError(\"Soup Not Created! Status Code: \", status_code)\n"
      ],
      "metadata": {
        "id": "eDvHU---D9Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_jobs(url: str) -> list:\n",
        "    \"\"\"Get job detail urls for all the jobs in current page(url).\n",
        "\n",
        "    Args:\n",
        "        url (str): Current page with jobs.\n",
        "\n",
        "    Returns:\n",
        "        list_data (list): List of job detail urls.\n",
        "    \"\"\"\n",
        "    soup = prepare_soup(url)\n",
        "    string_data = soup.find_all(\"script\", type=\"application/ld+json\")[0].text\n",
        "    json_data = json.loads(string_data)['itemListElement']\n",
        "    list_data = [data['url'] for data in json_data]\n",
        "\n",
        "    pages = soup.find('ul', 'pagination').find_all('li')[-2].find_all(['a', 'span'])[0].text\n",
        "    return (list_data, pages)\n"
      ],
      "metadata": {
        "id": "N6DoV6jLCbhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_job_details(soup: 'BeautifulSoup', url) -> dict:\n",
        "    \"\"\"Fetch details for each job.\n",
        "\n",
        "    Each dictionary contains details about only one job. Try Except \n",
        "    used to handle possible errors due to change in selectors.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup): Browsable bs4 object.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of job details.\n",
        "    \"\"\"\n",
        "    details = {}\n",
        "    try:\n",
        "        details['title'] = soup.find('h1', 'job-header__title').text\n",
        "    except AttributeError:\n",
        "        details['title'] = 'None'\n",
        "    try:\n",
        "        details['job_function'] = soup.find(\n",
        "            'div', 'hide-under-lg').find_all('h2')[1].text\n",
        "    except AttributeError:\n",
        "        details['job_function'] = 'None'\n",
        "    try:\n",
        "        details['location'] = soup.find('a', 'job-header__location').text\n",
        "    except AttributeError:\n",
        "        details['location'] = 'None'\n",
        "    try:\n",
        "        details['industry'] = soup.find('span',\n",
        "                                        'job-header__location').find('a').text\n",
        "    except AttributeError:\n",
        "        details['industry'] = 'None'\n",
        "    try:\n",
        "        details['description'] = soup.find_all(\n",
        "            'div', 'customer-card__content-segment')[0].find('p').text\n",
        "    except AttributeError:\n",
        "        details['description'] = 'None'\n",
        "    try:\n",
        "        details['qualifications'] = soup.find(\n",
        "            'div', 'description-content__content').text\n",
        "    except AttributeError:\n",
        "        details['qualifications'] = 'None'\n",
        "    details['hyperlink'] = url\n",
        "\n",
        "    return details\n",
        "\n"
      ],
      "metadata": {
        "id": "3npgQ0CPI683"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.brightermonday.co.ke/jobs\"\n",
        "\n",
        "def main(url):\n",
        "    page = 1\n",
        "    while True:\n",
        "        current_url = url+f'?page={page}'\n",
        "        page += 1\n",
        "        job_data = []\n",
        "        job_urls = fetch_jobs(current_url)[0]\n",
        "        total_pages = int(fetch_jobs(current_url)[1])\n",
        "        for job_url in job_urls:\n",
        "            soup = prepare_soup(job_url)\n",
        "            if soup != None:\n",
        "                job_details = fetch_job_details(soup, job_url)\n",
        "                job_data.append(job_details)\n",
        "\n",
        "                print(job_details)\n",
        "            else:\n",
        "                break\n",
        "        if page <= total_pages:\n",
        "            time.sleep(randint(1, 5))\n",
        "        else:\n",
        "            break\n",
        "    return job_data\n",
        "        \n",
        "main(url)"
      ],
      "metadata": {
        "id": "N_5cKJPpLYzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2EeuawPTzRHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}